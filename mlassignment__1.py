# -*- coding: utf-8 -*-
"""MLAssignment _1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HO_6StaHU5ApPUvwXdnqpdw4r1vMIovf

## PART@1
Multilinear Regression &amp; Polynomial Regression
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error

pip install kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("spscientist/students-performance-in-exams")

print("Path to dataset files:", path)

print(path)

"""## DATA PREPROCESSING"""

df = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')
print(df.head(10))

#EDA
df.isnull().sum()

df.duplicated().sum()

from scipy.stats import zscore

z_scores = df.select_dtypes(include='number').apply(zscore)
outliers = (abs(z_scores) > 3)  # Z-score > 3 is usually considered an outlier
print(outliers.sum())  # Total number of outliers per column

# Select only numeric columns
numeric_df = df.select_dtypes(include='number')

# Calculate Q1, Q3, IQR on numeric columns only
Q1 = numeric_df.quantile(0.25)
Q3 = numeric_df.quantile(0.75)
IQR = Q3 - Q1

# Detect outliers using IQR method
outliers = ((numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR)))

# Count total outliers per numeric column
print(outliers.sum())

# Select numeric columns
numeric_cols = ['math score', 'reading score', 'writing score']

plt.figure(figsize=(12, 6))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

df['parental level of education'] = le.fit_transform(df['parental level of education'])

print(df["parental level of education"])

df.head(10)

df_onehot = pd.get_dummies(df, columns=["gender"	,"race/ethnicity",
                                        "lunch",	"test preparation course"])
print(df_onehot.head())

print(df["gender"])

#Normalize or standardize features (optional but recommended).
# Selecting numerical features
num_features = ['math score', 'reading score', 'writing score']

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform
df_scaled = df.copy()
df_scaled[num_features] = scaler.fit_transform(df[num_features])

print(df_scaled.head())

df_encoded = pd.get_dummies(df, columns=['test preparation course', 'lunch', 'race/ethnicity', 'gender'], dtype=int)

"""## 2. Feature Engineering -"""

#Choose relevant features to predict math score.
# Only take numeric columns for correlation
correlation_matrix = df_encoded.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

selected_features = ['reading score', 'writing score',
                     'test preparation course_completed', 'lunch_standard']
target = 'math score'

X = df_encoded[selected_features]
y = df_encoded[target]

"""## MODEL BUILDING"""

#Split the data into training and test sets (80/20).
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform on train
X_test_scaled = scaler.transform(X_test)        # Only transform on te

# Initialize and train model
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)

linear_model.intercept_

linear_model.coef_

# Step 7 : predict model
y_pred = linear_model.predict(X_test)

y_pred

y_pred_linear = linear_model.predict(X_test)

# Predict and evaluate
y_pred = linear_model.predict(X_test_scaled)
print("Linear Regression MSE:", mean_squared_error(y_test, y_pred))
print("Linear Regression R2:", r2_score(y_test, y_pred))

"""## Polynomial Regression (degree 2 or 3)"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Degree of polynomial
degree = 2  # change to 3 if you want degree 3

# Create a pipeline that first creates polynomial features then fits linear regression
poly_model = make_pipeline(
    PolynomialFeatures(degree),
    StandardScaler(),
    LinearRegression()
)

# Train
poly_model.fit(X_train, y_train)

#Predict and evaluate
y_poly_pred = poly_model.predict(X_test)
print(f"Polynomial Regression (degree {degree}) MSE:", mean_squared_error(y_test, y_poly_pred))
print(f"Polynomial Regression (degree {degree}) R2:", r2_score(y_test, y_poly_pred))

poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)

"""## MODEL EVALUATION"""

plt.figure(figsize=(12, 5))

# Multilinear Regression plot
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_linear, alpha=0.7, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # y=x line
plt.title("Multilinear Regression: Actual vs Predicted")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")

# Polynomial Regression plot
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred_poly, alpha=0.7, color='green')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # y=x line
plt.title(f"Polynomial Regression (degree {degree}): Actual vs Predicted")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")

plt.tight_layout()
plt.show()

